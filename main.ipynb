{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main notebook I use for testing code and creating graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# check pytorch version\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "from torch_geometric.nn import GCN\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import ReLU, Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GINConv, global_add_pool, Sequential\n",
    "from torch_geometric.nn import GATConv, GPSConv, GINEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import OneHotDegree\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the datasets\n",
    "cora = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "imdb = TUDataset(root=f'/tmp/IMDB-BINARY', name='IMDB-BINARY')\n",
    "enzymes = TUDataset(root=f'/tmp/ENZYMES', name='ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a dict that we will use to store performance for each of our models on each dataset\n",
    "scores = {\"Cora\": {}, \"Imdb\": {}, \"Enzymes\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a basic GCN for node classification\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(cora.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, cora.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model, data, and optimizer setup in torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = cora[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 1.9454\n",
      "Epoch 020, Loss: 0.1078\n",
      "Epoch 040, Loss: 0.0161\n",
      "Epoch 060, Loss: 0.0151\n",
      "Epoch 080, Loss: 0.0174\n",
      "Epoch 100, Loss: 0.0161\n",
      "Epoch 120, Loss: 0.0143\n",
      "Epoch 140, Loss: 0.0129\n"
     ]
    }
   ],
   "source": [
    "# train model for 150 epochs\n",
    "\n",
    "model.train()\n",
    "for epoch in range(151):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8010\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "scores[\"Cora\"][\"GCN\"] = acc\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a basic GCN for graph classification\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.fc = torch.nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper fuction to make node features for the IMDB dataset\n",
    "# We just represent each node with its degree\n",
    "def create_degree_features(data):\n",
    "    num_nodes = data.num_nodes\n",
    "    degrees = torch.bincount(data.edge_index[0], minlength=num_nodes)\n",
    "    \n",
    "    feature_vector = degrees.view(-1, 1).float()\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data, train the model, and evaluate it for graph classification\n",
    "def train_and_evaluate(dataset, epochs = 150):\n",
    "\n",
    "    # Loading Data\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCN(dataset.num_node_features or 1, dataset.num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # if no node features we add in the degrees\n",
    "            if data.x is None:\n",
    "                data.x = create_degree_features(data).to(device)\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        # if no node features we add in the degrees\n",
    "        if data.x is None:\n",
    "            data.x = create_degree_features(data).to(device)\n",
    "\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on Imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6883\n",
      "Epoch 20, Loss: 0.5198\n",
      "Epoch 40, Loss: 0.6484\n",
      "Epoch 60, Loss: 0.4552\n",
      "Epoch 80, Loss: 0.4928\n",
      "Epoch 100, Loss: 0.5091\n",
      "Epoch 120, Loss: 0.4440\n",
      "Epoch 140, Loss: 0.5197\n",
      "Accuracy: 0.7100\n"
     ]
    }
   ],
   "source": [
    "scores[\"Imdb\"][\"GCN\"] = train_and_evaluate(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on Enzymes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.7601\n",
      "Epoch 20, Loss: 1.8847\n",
      "Epoch 40, Loss: 1.6417\n",
      "Epoch 60, Loss: 1.7583\n",
      "Epoch 80, Loss: 1.6913\n",
      "Epoch 100, Loss: 1.6136\n",
      "Epoch 120, Loss: 1.5813\n",
      "Epoch 140, Loss: 1.6242\n",
      "Accuracy: 0.2667\n"
     ]
    }
   ],
   "source": [
    "scores[\"Enzymes\"][\"GCN\"] = train_and_evaluate(enzymes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the scores so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cora': {'GCN': 0.801},\n",
       " 'Imdb': {'GCN': 0.71},\n",
       " 'Enzymes': {'GCN': 0.26666666666666666}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIN Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to mind max degree in dataset\n",
    "def find_max_degree(dataset):\n",
    "    max_deg = -1\n",
    "    for data in dataset:\n",
    "        max_deg = max(max_deg, data.edge_index[0].bincount().max().item())\n",
    "    return max_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max of this dataset\n",
    "cora_max = find_max_degree(cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a transformed dataset with the one hot encoded degree features\n",
    "transformed_cora = Planetoid(root='/tmp/Cora', name='Cora', transform=OneHotDegree(cora_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a GIN for node classification\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(transformed_cora.num_node_features, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 16)\n",
    "        )\n",
    "        self.mlp2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, transformed_cora.num_classes)\n",
    "        )\n",
    "        \n",
    "        self.conv1 = GINConv(self.mlp1, eps=0.0, train_eps=True)\n",
    "        self.conv2 = GINConv(self.mlp2, eps=0.0, train_eps=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model, data, and optimizer setup in torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GIN().to(device)\n",
    "data = transformed_cora[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 1.9610\n",
      "Epoch 020, Loss: 0.0504\n",
      "Epoch 040, Loss: 0.0004\n",
      "Epoch 060, Loss: 0.0002\n",
      "Epoch 080, Loss: 0.0003\n",
      "Epoch 100, Loss: 0.0003\n",
      "Epoch 120, Loss: 0.0004\n",
      "Epoch 140, Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# train model for 150 epochs\n",
    "\n",
    "model.train()\n",
    "for epoch in range(151):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7620\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "scores[\"Cora\"][\"GIN\"] = acc\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a GIN for node classification\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_node_features, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 16)\n",
    "        )\n",
    "        self.mlp2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 16)\n",
    "        )\n",
    "\n",
    "        self.conv1 = GINConv(self.mlp1, eps=0.0, train_eps=True)\n",
    "        self.conv2 = GINConv(self.mlp2, eps=0.0, train_eps=True)\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max degrees for our datasets\n",
    "imdb_max = find_max_degree(imdb)\n",
    "enzymes_max = find_max_degree(enzymes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a transformed datasets with the one hot encoded degree features\n",
    "transformed_imdb = TUDataset(root='/tmp/IMDB', name='IMDB-BINARY', transform=OneHotDegree(imdb_max))\n",
    "transformed_enzymes = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', transform=OneHotDegree(enzymes_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data, train the model, and evaluate it for graph classification\n",
    "def train_and_evaluate(dataset, epochs = 150):\n",
    "\n",
    "    # Loading Data\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GIN(dataset.num_node_features or 1, dataset.num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5298\n",
      "Epoch 20, Loss: 0.4502\n",
      "Epoch 40, Loss: 0.3660\n",
      "Epoch 60, Loss: 0.3644\n",
      "Epoch 80, Loss: 0.2267\n",
      "Epoch 100, Loss: 0.3584\n",
      "Epoch 120, Loss: 0.3621\n",
      "Epoch 140, Loss: 0.4540\n",
      "Accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(transformed_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5993\n",
      "Epoch 20, Loss: 0.7544\n",
      "Epoch 40, Loss: 0.4139\n",
      "Epoch 60, Loss: 0.3040\n",
      "Epoch 80, Loss: 0.2289\n",
      "Epoch 100, Loss: 0.3539\n",
      "Epoch 120, Loss: 0.1734\n",
      "Epoch 140, Loss: 0.2893\n",
      "Accuracy: 0.7100\n"
     ]
    }
   ],
   "source": [
    "# Test model on Imdb dataset\n",
    "scores[\"Imdb\"][\"GIN\"] = train_and_evaluate(transformed_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9343\n",
      "Epoch 20, Loss: 1.4890\n",
      "Epoch 40, Loss: 1.2539\n",
      "Epoch 60, Loss: 1.2368\n",
      "Epoch 80, Loss: 0.9421\n",
      "Epoch 100, Loss: 0.9144\n",
      "Epoch 120, Loss: 1.0295\n",
      "Epoch 140, Loss: 0.8362\n",
      "Accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "# Test model on Enzymes dataset\n",
    "scores[\"Enzymes\"][\"GIN\"] = train_and_evaluate(transformed_enzymes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cora': {'GCN': 0.801, 'GIN': 0.762},\n",
       " 'Imdb': {'GCN': 0.71, 'GIN': 0.71},\n",
       " 'Enzymes': {'GCN': 0.26666666666666666, 'GIN': 0.4}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT Optimal Number of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node classification GCN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a new GCN that can handle a dynamic number of layers\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we turn everything into neat helper functions since we are going to be training a lot of models here\n",
    "\n",
    "def train_model(model, data, optimizer, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(data).argmax(dim=1)\n",
    "        train_correct = (pred[data.train_mask] == data.y[data.train_mask]).sum()\n",
    "        val_correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "        test_correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        \n",
    "        train_acc = float(train_correct) / int(data.train_mask.sum())\n",
    "        val_acc = float(val_correct) / int(data.val_mask.sum())\n",
    "        test_acc = float(test_correct) / int(data.test_mask.sum())\n",
    "        \n",
    "    return train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 1 layers\n",
      "Training model with 2 layers\n",
      "Training model with 3 layers\n",
      "Training model with 4 layers\n",
      "Training model with 5 layers\n",
      "Training model with 6 layers\n",
      "Training model with 7 layers\n",
      "Training model with 8 layers\n",
      "Training model with 9 layers\n",
      "Training model with 10 layers\n",
      "Training model with 11 layers\n",
      "Training model with 12 layers\n",
      "Training model with 13 layers\n",
      "Training model with 14 layers\n",
      "Training model with 15 layers\n",
      "\n",
      "==================================================\n",
      "Summary of Results\n",
      "==================================================\n",
      "\n",
      "Number of Layers | Train Acc | Val Acc | Test Acc\n",
      "--------------------------------------------------\n",
      "             1 | 1.0000 | 0.7800 | 0.8020\n",
      "             2 | 1.0000 | 0.7740 | 0.7990\n",
      "             3 | 1.0000 | 0.7840 | 0.7870\n",
      "             4 | 1.0000 | 0.7400 | 0.7420\n",
      "             5 | 1.0000 | 0.7460 | 0.7350\n",
      "             6 | 1.0000 | 0.7260 | 0.7400\n",
      "             7 | 1.0000 | 0.7200 | 0.7100\n",
      "             8 | 1.0000 | 0.6640 | 0.6840\n",
      "             9 | 0.9000 | 0.5500 | 0.5780\n",
      "            10 | 0.9714 | 0.5820 | 0.6030\n",
      "            11 | 0.9000 | 0.5780 | 0.5850\n",
      "            12 | 0.6929 | 0.4100 | 0.4520\n",
      "            13 | 0.3000 | 0.1700 | 0.1370\n",
      "            14 | 0.2714 | 0.1960 | 0.1620\n",
      "            15 | 0.2857 | 0.2060 | 0.1760\n"
     ]
    }
   ],
   "source": [
    "# Test different numbers of layers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = cora[0].to(device)\n",
    "\n",
    "results = {}\n",
    "for num_layers in range(1, 16):\n",
    "    print(f\"Training model with {num_layers} layers\")\n",
    "    \n",
    "    model = GCN(\n",
    "        in_channels=cora.num_node_features,\n",
    "        hidden_channels=16,\n",
    "        out_channels=cora.num_classes,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    model = train_model(model, data, optimizer)\n",
    "    \n",
    "    train_acc, val_acc, test_acc = evaluate_model(model, data)\n",
    "    \n",
    "    results[num_layers] = {\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary of Results\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNumber of Layers | Train Acc | Val Acc | Test Acc\")\n",
    "print(\"-\"*50)\n",
    "for layers in range(1, 16):\n",
    "    r = results[layers]\n",
    "    print(f\"{layers:14d} | {r['train_acc']:.4f} | {r['val_acc']:.4f} | {r['test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node classification GAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(GATConv(\n",
    "            in_channels, \n",
    "            hidden_channels, \n",
    "            heads=heads, \n",
    "            concat=True, \n",
    "        ))\n",
    "        \n",
    "        hidden_in_channels = hidden_channels * heads\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(\n",
    "                hidden_in_channels,\n",
    "                hidden_channels,\n",
    "                heads=heads,\n",
    "                concat=True,\n",
    "            ))\n",
    "            \n",
    "        self.convs.append(GATConv(\n",
    "            hidden_in_channels,\n",
    "            out_channels,\n",
    "            heads=heads,\n",
    "            concat=False,\n",
    "        ))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = F.elu(conv(x, edge_index)) # Note: uses elu instead of relu\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 1 layers\n",
      "Training model with 2 layers\n",
      "Training model with 3 layers\n",
      "Training model with 4 layers\n",
      "Training model with 5 layers\n",
      "Training model with 6 layers\n",
      "Training model with 7 layers\n",
      "Training model with 8 layers\n",
      "Training model with 9 layers\n",
      "Training model with 10 layers\n",
      "Training model with 11 layers\n",
      "Training model with 12 layers\n",
      "Training model with 13 layers\n",
      "Training model with 14 layers\n",
      "Training model with 15 layers\n",
      "\n",
      "==================================================\n",
      "Summary of Results\n",
      "==================================================\n",
      "\n",
      "Number of Layers | Train Acc | Val Acc | Test Acc\n",
      "--------------------------------------------------\n",
      "             1 | 1.0000 | 0.7180 | 0.7690\n",
      "             2 | 1.0000 | 0.7200 | 0.7520\n",
      "             3 | 1.0000 | 0.7740 | 0.8000\n",
      "             4 | 1.0000 | 0.7700 | 0.7820\n",
      "             5 | 1.0000 | 0.7380 | 0.7610\n",
      "             6 | 1.0000 | 0.7640 | 0.7480\n",
      "             7 | 1.0000 | 0.7540 | 0.7570\n",
      "             8 | 1.0000 | 0.7540 | 0.7760\n",
      "             9 | 1.0000 | 0.7540 | 0.7790\n",
      "            10 | 1.0000 | 0.7540 | 0.7710\n",
      "            11 | 1.0000 | 0.7320 | 0.7460\n",
      "            12 | 1.0000 | 0.7500 | 0.7590\n",
      "            13 | 0.9214 | 0.7060 | 0.7090\n",
      "            14 | 1.0000 | 0.6780 | 0.6950\n",
      "            15 | 0.9786 | 0.7060 | 0.7130\n"
     ]
    }
   ],
   "source": [
    "# Test different numbers of layers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = cora[0].to(device)\n",
    "\n",
    "results = {}\n",
    "for num_layers in range(1, 16):\n",
    "    print(f\"Training model with {num_layers} layers\")\n",
    "    \n",
    "    model = GAT(\n",
    "        in_channels=cora.num_node_features,\n",
    "        hidden_channels=16,\n",
    "        out_channels=cora.num_classes,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    model = train_model(model, data, optimizer)\n",
    "    \n",
    "    train_acc, val_acc, test_acc = evaluate_model(model, data)\n",
    "    \n",
    "    results[num_layers] = {\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary of Results\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNumber of Layers | Train Acc | Val Acc | Test Acc\")\n",
    "print(\"-\"*50)\n",
    "for layers in range(1, 16):\n",
    "    r = results[layers]\n",
    "    print(f\"{layers:14d} | {r['train_acc']:.4f} | {r['val_acc']:.4f} | {r['test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(\n",
    "            cora.num_node_features, \n",
    "            16, \n",
    "            heads=heads, \n",
    "            concat=True,\n",
    "        )\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            16 * heads,\n",
    "            cora.num_classes,\n",
    "            heads=heads, \n",
    "            concat=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.conv1(x, edge_index))  # Note: uses elu not relu\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model, data, and optimizer setup in torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAT().to(device)\n",
    "data = cora[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 1.9569\n",
      "Epoch 020, Loss: 0.0012\n",
      "Epoch 040, Loss: 0.0031\n",
      "Epoch 060, Loss: 0.0083\n",
      "Epoch 080, Loss: 0.0066\n",
      "Epoch 100, Loss: 0.0060\n",
      "Epoch 120, Loss: 0.0057\n",
      "Epoch 140, Loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "# train model for 150 epochs\n",
    "\n",
    "model.train()\n",
    "for epoch in range(151):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7620\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "scores[\"Cora\"][\"GAT\"] = acc\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a basic GCN for graph classification\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, heads = 4):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(\n",
    "            num_node_features, \n",
    "            16, \n",
    "            heads=heads, \n",
    "            concat=True,\n",
    "        )\n",
    "        self.conv2 = GATConv(\n",
    "            16 * heads,\n",
    "            16,\n",
    "            heads=heads, \n",
    "            concat=False,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper fuction to make node features for the IMDB dataset\n",
    "# We just represent each node with its degree\n",
    "def create_degree_features(data):\n",
    "    num_nodes = data.num_nodes\n",
    "    degrees = torch.bincount(data.edge_index[0], minlength=num_nodes)\n",
    "    \n",
    "    feature_vector = degrees.view(-1, 1).float()\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data, train the model, and evaluate it for graph classification\n",
    "def train_and_evaluate(dataset, epochs = 150):\n",
    "\n",
    "    # Loading Data\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GAT(dataset.num_node_features or 1, dataset.num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # if no node features we add in the degrees\n",
    "            if data.x is None:\n",
    "                data.x = create_degree_features(data).to(device)\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        # if no node features we add in the degrees\n",
    "        if data.x is None:\n",
    "            data.x = create_degree_features(data).to(device)\n",
    "\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6996\n",
      "Epoch 20, Loss: 0.5440\n",
      "Epoch 40, Loss: 0.5227\n",
      "Epoch 60, Loss: 0.5801\n",
      "Epoch 80, Loss: 0.4989\n",
      "Epoch 100, Loss: 0.5632\n",
      "Epoch 120, Loss: 0.6200\n",
      "Epoch 140, Loss: 0.4845\n",
      "Accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "#Test model on Imdb dataset\n",
    "scores[\"Imdb\"][\"GAT\"] = train_and_evaluate(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.8117\n",
      "Epoch 20, Loss: 1.6811\n",
      "Epoch 40, Loss: 1.7913\n",
      "Epoch 60, Loss: 1.7828\n",
      "Epoch 80, Loss: 1.6240\n",
      "Epoch 100, Loss: 1.7801\n",
      "Epoch 120, Loss: 1.4785\n",
      "Epoch 140, Loss: 1.4140\n",
      "Accuracy: 0.3083\n"
     ]
    }
   ],
   "source": [
    "#Test model on Enzymes dataset\n",
    "scores[\"Enzymes\"][\"GAT\"] = train_and_evaluate(enzymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graphGPS Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPS(torch.nn.Module):\n",
    "    def __init__(self, heads=1):\n",
    "        super().__init__()\n",
    "        nn1 = Sequential(\n",
    "                Linear(16, 16),\n",
    "                ReLU(),\n",
    "                Linear(16, 16),\n",
    "        )\n",
    "        nn2 = Sequential(\n",
    "                Linear(16, 16),\n",
    "                ReLU(),\n",
    "                Linear(16, 16),\n",
    "        )\n",
    "        self.conv1 = GPSConv(\n",
    "            channels=16,\n",
    "            conv=GINEConv(nn1), \n",
    "            heads=heads, \n",
    "        )\n",
    "        self.conv2 = GPSConv(\n",
    "            channels=cora.num_classes,   \n",
    "            conv=GINEConv(nn2), \n",
    "            heads=heads,\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[181], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get the model, data, and optimizer setup in torch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m cora[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[180], line 4\u001b[0m, in \u001b[0;36mGPS.__init__\u001b[1;34m(self, heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     nn1 \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     nn2 \u001b[38;5;241m=\u001b[39m Sequential(\n\u001b[0;32m     10\u001b[0m             Linear(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[0;32m     11\u001b[0m             ReLU(),\n\u001b[0;32m     12\u001b[0m             Linear(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m GPSConv(\n\u001b[0;32m     15\u001b[0m         channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     16\u001b[0m         conv\u001b[38;5;241m=\u001b[39mGINEConv(nn1), \n\u001b[0;32m     17\u001b[0m         heads\u001b[38;5;241m=\u001b[39mheads, \n\u001b[0;32m     18\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Get the model, data, and optimizer setup in torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPS().to(device)\n",
    "data = cora[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[173], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m151\u001b[39m):\n\u001b[0;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 6\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[171], line 20\u001b[0m, in \u001b[0;36mGPS.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     18\u001b[0m     x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch_geometric\\nn\\conv\\gps_conv.py:142\u001b[0m, in \u001b[0;36mGPSConv.forward\u001b[1;34m(self, x, edge_index, batch, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m hs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Local MPNN.\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x, edge_index, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(h, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    144\u001b[0m     h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.__init__\u001b[1;34m(self, in_channels, out_channels, improved, cached, add_self_loops, normalize, bias, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_adj_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mweight_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglorot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_channels))\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_channels, out_channels, bias, weight_initializer, bias_initializer)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_initializer \u001b[38;5;241m=\u001b[39m weight_initializer\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_initializer \u001b[38;5;241m=\u001b[39m bias_initializer\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43min_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_channels, in_channels))\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# train model for 150 epochs\n",
    "\n",
    "model.train()\n",
    "for epoch in range(151):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "scores[\"Cora\"][\"GPS\"] = acc\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Embedding,\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, GPSConv, global_add_pool\n",
    "from torch_geometric.nn.attention import PerformerAttention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPS(torch.nn.Module):\n",
    "    def __init__(self, channels: int, pe_dim: int, num_layers: int,\n",
    "                 attn_type: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_emb = Embedding(28, channels - pe_dim)\n",
    "        self.pe_lin = Linear(20, pe_dim)\n",
    "        self.pe_norm = BatchNorm1d(20)\n",
    "        self.edge_emb = Embedding(4, channels)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(channels, channels),\n",
    "                ReLU(),\n",
    "                Linear(channels, channels),\n",
    "            )\n",
    "            conv = GPSConv(channels, GINEConv(nn), heads=4,\n",
    "                           attn_type=attn_type)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels // 2),\n",
    "            ReLU(),\n",
    "            Linear(channels // 2, channels // 4),\n",
    "            ReLU(),\n",
    "            Linear(channels // 4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pe, edge_index, edge_attr, batch):\n",
    "        x_pe = self.pe_norm(pe)\n",
    "        x = torch.cat((self.node_emb(x.squeeze(-1)), self.pe_lin(x_pe)), 1)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, batch, edge_attr=edge_attr)\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper fuction to make node features for the IMDB dataset\n",
    "# We just represent each node with its degree\n",
    "def create_degree_features(data):\n",
    "    num_nodes = data.num_nodes\n",
    "    degrees = torch.bincount(data.edge_index[0], minlength=num_nodes)\n",
    "    \n",
    "    feature_vector = degrees.view(-1, 1).float()\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data, train the model, and evaluate it for graph classification\n",
    "def train_and_evaluate(dataset, epochs = 150):\n",
    "\n",
    "    # Loading Data\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GPS(16, 16, 2, \"multihead\").to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # if no node features we add in the degrees\n",
    "            if data.x is None:\n",
    "                data.x = create_degree_features(data).to(device)\n",
    "\n",
    "            out = model(data.x, data.pe, data.edge_index, data.edge_attr,\n",
    "                    data.batch)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        # if no node features we add in the degrees\n",
    "        if data.x is None:\n",
    "            data.x = create_degree_features(data).to(device)\n",
    "\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "        total += data.num_graphs\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'pe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[204], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[203], line 27\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m create_degree_features(data)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 27\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr,\n\u001b[0;32m     28\u001b[0m         data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch_geometric\\data\\data.py:561\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\quent\\anaconda3\\envs\\DSC180A\\lib\\site-packages\\torch_geometric\\data\\storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'pe'"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC180A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
